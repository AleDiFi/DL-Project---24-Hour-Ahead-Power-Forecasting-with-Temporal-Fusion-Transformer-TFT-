# Development Log - Solar Power Forecasting with TFT

**Progetto:** Previsione Produzione Fotovoltaica 24h-ahead con Temporal Fusion Transformer  
**Data Inizio:** Novembre 2025  
**Autore:** Alessandro Di Filippo  
**Hardware:** NVIDIA GeForce RTX 4060 (8GB VRAM)

---

## üìã Indice

1. [Panoramica Progetto](#panoramica-progetto)
2. [Architettura Iniziale](#architettura-iniziale)
3. [Problemi Identificati](#problemi-identificati)
4. [Ottimizzazioni Implementate](#ottimizzazioni-implementate)
5. [Configurazione GPU](#configurazione-gpu)
6. [Configurazione Finale](#configurazione-finale)
7. [Tracciamento Modifiche](#tracciamento-modifiche)

---

## üéØ Panoramica Progetto

### Obiettivo
Sviluppare un modello di Deep Learning basato su **Temporal Fusion Transformer (TFT)** per prevedere la produzione di energia fotovoltaica con **24 ore di anticipo**, utilizzando:
- **Encoder**: 168 ore (1 settimana) di dati storici
- **Decoder**: 24 ore di previsioni future

### Dataset
- **Periodo**: Luglio 2010 - Giugno 2012 (2 anni)
- **Campioni totali**: 17,317 osservazioni orarie
- **Features**:
  - Target: `power_kw` (produzione fotovoltaica)
  - Temporali: `hour`, `day_of_month`, `month`, `day_of_week`
  - Meteorologiche: `temp`, `Dni`, `Ghi`, `humidity`, `clouds_all`, `wind_speed`, `pressure`, `rain_1h`

### Split Dati
- **Training**: ~15,157 campioni (primi 21 mesi)
- **Validation**: 2,160 campioni (ultimi 3 mesi)

---

## üèóÔ∏è Architettura Iniziale

### Configurazione TFT Originale (Pre-ottimizzazione)

```python
# Configurazione iniziale (problematica)
tft = TemporalFusionTransformer.from_dataset(
    training_dataset,
    learning_rate=0.03,              # ‚ùå Troppo alto
    hidden_size=128,                  # ‚ùå Troppo complesso
    lstm_layers=2,                    # ‚ùå Rischio overfitting
    attention_head_size=4,
    dropout=0.1,                      # ‚ùå Regolarizzazione insufficiente
    hidden_continuous_size=16,
    output_size=7,
    loss=QuantileLoss(),
)
```

### Training Configuration Iniziale

```python
# Early stopping
patience=10                           # ‚ùå Troppo impaziente

# Trainer
max_epochs=50                         # ‚ùå Potrebbe essere insufficiente
gradient_clip_val=0.1                 # ‚ùå Troppo aggressivo

# Normalizzazione
transformation="softplus"             # ‚ùå Problematico con molti zeri
```

---

## ‚ö†Ô∏è Problemi Identificati

### Problema 1: Modello Non Apprende
**Sintomi:**
- Loss costante durante il training
- Predizioni tutte a zero
- Training si interrompe dopo 10 epoch (early stopping)
- Nessun miglioramento nelle metriche

**Diagnosi:**
```
Epoch 1-10: Loss = 2.5634 (costante)
Predictions: [0, 0, 0, 0, 0, 0, 0, 0, ...]
MAE: molto alto
R¬≤: negativo o vicino a 0
```

### Problema 2: Distribuzione Dati Problematica
**Analisi diagnostica rivel√≤:**
- **~40% valori = 0** (produzione notturna ore 20:00-6:00)
- **Distribuzione bimodale** (giorno/notte)
- Normalizzazione `softplus` inadatta per dati con molti zeri
- Gradiente sparisce nelle ore notturne

**Grafico distribuzione oraria:**
```
00:00 |   0.00 kW |
01:00 |   0.00 kW |
...
06:00 |   5.23 kW | ‚ñà‚ñà
07:00 |  15.67 kW | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
08:00 |  28.45 kW | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
...
12:00 |  45.89 kW | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
...
19:00 |   8.12 kW | ‚ñà‚ñà‚ñà‚ñà
20:00 |   0.00 kW |
```

### Problema 3: Iperparametri Sub-ottimali

| Parametro | Valore Iniziale | Problema |
|-----------|----------------|----------|
| `learning_rate` | 0.03 | Troppo alto ‚Üí oscillazioni, non converge |
| `gradient_clip_val` | 0.1 | Troppo aggressivo ‚Üí gradiente scompare |
| `hidden_size` | 128 | Troppo complesso ‚Üí overfitting |
| `lstm_layers` | 2 | Capacit√† eccessiva per dataset piccolo |
| `dropout` | 0.1 | Regolarizzazione insufficiente |
| `patience` | 10 | Early stopping troppo veloce |
| `max_epochs` | 50 | Potenzialmente insufficiente |

---

## üîß Ottimizzazioni Implementate

### Ottimizzazione 1: Learning Rate Ridotto

**Modifica:**
```python
# PRIMA
learning_rate=0.03

# DOPO
learning_rate=0.001  # Ridotto di 30x
```

**Motivazione:**
- Learning rate alto causa oscillazioni nella loss
- Gradiente "salta" sopra i minimi locali
- Convergenza instabile

**Impatto atteso:**
- Convergenza pi√π stabile e graduale
- Loss che diminuisce effettivamente
- Migliore apprendimento delle feature sottili

---

### Ottimizzazione 2: Architettura Semplificata

**Modifica:**
```python
# PRIMA
hidden_size=128
lstm_layers=2
hidden_continuous_size=16

# DOPO
hidden_size=64          # Ridotto del 50%
lstm_layers=1           # Ridotto del 50%
hidden_continuous_size=8  # Ridotto del 50%
```

**Motivazione:**
- Dataset relativamente piccolo (17k campioni)
- Modello complesso ‚Üí overfitting
- Capacit√† eccessiva per il task

**Impatto atteso:**
- Riduzione overfitting
- Generalizzazione migliore
- Training pi√π veloce
- Meno parametri da ottimizzare

**Riduzione parametri:**
```
PRIMA: ~450,000 parametri trainable
DOPO:  ~120,000 parametri trainable (-73%)
```

---

### Ottimizzazione 3: Regolarizzazione Aumentata

**Modifica:**
```python
# PRIMA
dropout=0.1

# DOPO
dropout=0.2  # Raddoppiato
```

**Motivazione:**
- Dropout basso ‚Üí il modello memorizza invece di generalizzare
- Necessario pi√π "noise" durante training
- Prevenire overfitting su pattern specifici

**Impatto atteso:**
- Maggiore robustezza
- Migliore performance su dati mai visti
- Riduzione gap train/validation loss

---

### Ottimizzazione 4: Gradient Clipping Rilassato

**Modifica:**
```python
# PRIMA
gradient_clip_val=0.1

# DOPO
gradient_clip_val=1.0  # Aumentato di 10x
```

**Motivazione:**
- Clipping troppo aggressivo "taglia" gradiente utile
- Loss non diminuisce perch√© update troppo piccoli
- Backpropagation inefficace

**Impatto atteso:**
- Gradienti possono fluire meglio
- Update dei pesi pi√π significativi
- Loss che effettivamente diminuisce

---

### Ottimizzazione 5: Normalizzazione Modificata

**Modifica:**
```python
# PRIMA
target_normalizer=GroupNormalizer(
    groups=["group_id"], 
    transformation="softplus"
)

# DOPO
target_normalizer=GroupNormalizer(
    groups=["group_id"], 
    transformation=None  # Normalizzazione standard
)
```

**Motivazione:**
- **Softplus**: `f(x) = log(1 + exp(x))` ‚Üí problematico quando molti valori = 0
- Gradiente quasi zero per produzione notturna (40% dei dati)
- Modello "ignora" ore notturne durante training

**Confronto trasformazioni:**
```python
# Softplus con valori piccoli/zero
softplus(0)     = 0.693  # Non preserva zero
softplus(0.01)  = 0.698  # Poco sensibile a piccole variazioni

# None (normalizzazione standard)
normalize(0)    = 0      # Preserva zero
normalize(0.01) = 0.001  # Proporzionale
```

**Impatto atteso:**
- Gradiente significativo anche per valori bassi
- Modello apprende pattern giorno/notte
- Predizioni non tutte a zero

---

### Ottimizzazione 6: Training Prolungato

**Modifica:**
```python
# PRIMA
max_epochs=50
patience=10

# DOPO
max_epochs=100  # Raddoppiato
patience=20     # Raddoppiato
```

**Motivazione:**
- Learning rate ridotto ‚Üí convergenza pi√π lenta
- Early stopping troppo veloce bloccava apprendimento
- Modello necessita pi√π tempo con LR basso

**Impatto atteso:**
- Maggiore tempo per raggiungere convergenza
- Loss continua a diminuire oltre epoch 10
- Migliore ottimizzazione dei pesi

---

### Ottimizzazione 7: Cella Diagnostica

**Aggiunta nuova cella di analisi (Sezione 5.1):**
```python
# Analizza distribuzione target
print("STATISTICHE GLOBALI")
print(f"Valori = 0: {zeros_count} ({zeros_pct:.2f}%)")

# Grafico produzione per ora del giorno
hourly_avg = data.groupby('hour')['power_kw'].mean()
# ASCII bar chart

# Plot: istogramma + boxplot
```

**Motivazione:**
- Identificare problemi nei dati prima del training
- Visualizzare distribuzione giorno/notte
- Verificare similarit√† train/validation

**Output esempio:**
```
STATISTICHE GLOBALI
============================================================
Min: 0.0000 kW
Max: 58.4500 kW
Mean: 18.2340 kW
Median: 15.6700 kW
Std: 19.8234 kW

Valori = 0: 6,927 (40.00%)
Valori > 0: 10,390 (60.00%)

PRODUZIONE MEDIA PER ORA DEL GIORNO
============================================================
00:00 |   0.00 kW |
...
12:00 |  45.89 kW | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
...
```

---

## üíª Configurazione GPU

### Problema Iniziale: CPU-Only
```
PyTorch version: 2.9.1+cpu
CUDA disponibile: False
Device: CPU
Training time: ~1-2 ore per 100 epochs
```

### Soluzione Implementata

**Step 1: Verifica compatibilit√†**
- Python 3.13 + CUDA 11.8 ‚úì
- NVIDIA GeForce RTX 4060 presente ‚úì

**Step 2: Reinstallazione PyTorch**
```powershell
# Disinstalla versione CPU
pip uninstall torch torchvision torchaudio -y

# Installa versione CUDA 11.8 (compatibile con Python 3.13)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

**Step 3: Fix NumPy compatibility**
```powershell
# NumPy 2.3.x causava DLL errors
pip install "numpy>=1.24,<2.3" --force-reinstall
```

**Step 4: Restart kernel**
- VS Code: "Restart Kernel" per ricaricare PyTorch

### Configurazione Finale GPU

```
============================================================
VERIFICA CONFIGURAZIONE GPU/CUDA
============================================================
PyTorch version: 2.7.1+cu118
CUDA disponibile: True
CUDA version: 11.8
Numero GPU disponibili: 1
GPU attiva: 0
Nome GPU: NVIDIA GeForce RTX 4060
Memoria GPU totale: 8.00 GB

‚úì Test GPU riuscito! Tensor creato su: cuda:0
============================================================
```

### Performance Gain
```
CPU (prima):  ~1-2 ore per 100 epochs
GPU (dopo):   ~10-15 minuti per 100 epochs
Speedup:      ~6-12x pi√π veloce! üöÄ
```

---

## ‚öôÔ∏è Configurazione Finale

### Modello TFT Ottimizzato

```python
tft = TemporalFusionTransformer.from_dataset(
    training_dataset,
    # Ottimizzazione 1: Learning rate ridotto
    learning_rate=0.001,           # ‚úì 0.03 ‚Üí 0.001 (stabile)
    
    # Ottimizzazione 2: Architettura semplificata
    hidden_size=64,                # ‚úì 128 ‚Üí 64 (meno overfitting)
    lstm_layers=1,                 # ‚úì 2 ‚Üí 1 (pi√π semplice)
    hidden_continuous_size=8,      # ‚úì 16 ‚Üí 8 (proporzionale)
    
    # Ottimizzazione 3: Regolarizzazione aumentata
    dropout=0.2,                   # ‚úì 0.1 ‚Üí 0.2 (migliore generalizzazione)
    
    # Parametri mantenuti
    attention_head_size=4,
    output_size=7,                 # 7 quantili per intervalli confidenza
    loss=QuantileLoss(),
    log_interval=10,
    reduce_on_plateau_patience=4,
)
```

### TimeSeriesDataSet Configuration

```python
training_dataset = TimeSeriesDataSet(
    training_data,
    time_idx="time_idx",
    target="power_kw",
    group_ids=["group_id"],
    
    # Finestre temporali
    max_encoder_length=168,        # 1 settimana contesto
    max_prediction_length=24,      # 24 ore previsione
    
    # Feature categoriche statiche
    static_categoricals=["group_id"],
    
    # Feature note nel futuro
    time_varying_known_reals=[
        'hour', 'day_of_month', 'month', 'day_of_week',  # Temporali
        'temp', 'Dni', 'Ghi', 'humidity', 'clouds_all',  # Meteo
        'wind_speed', 'pressure', 'rain_1h'
    ],
    
    # Feature sconosciute (solo storico)
    time_varying_unknown_reals=["power_kw"],
    
    # Ottimizzazione 5: Normalizzazione modificata
    target_normalizer=GroupNormalizer(
        groups=["group_id"], 
        transformation=None         # ‚úì "softplus" ‚Üí None
    ),
    
    add_relative_time_idx=True,
    add_target_scales=True,
    add_encoder_length=True,
)
```

### Training Configuration

```python
# Callbacks
early_stop_callback = EarlyStopping(
    monitor="val_loss",
    min_delta=1e-4,
    patience=20,                   # ‚úì 10 ‚Üí 20 (pi√π paziente)
    verbose=True,
    mode="min"
)

# Trainer
trainer = pl.Trainer(
    max_epochs=100,                # ‚úì 50 ‚Üí 100 (pi√π tempo)
    accelerator="auto",            # ‚úì Usa GPU se disponibile
    devices=1,
    gradient_clip_val=1.0,         # ‚úì 0.1 ‚Üí 1.0 (meno aggressivo)
    callbacks=[early_stop_callback, lr_monitor],
    logger=logger,
    enable_progress_bar=True,
    log_every_n_steps=10,
)
```

### DataLoaders

```python
batch_size = 64
num_workers = 0  # 0 per Windows (evita problemi multiprocessing)

train_dataloader = training_dataset.to_dataloader(
    train=True, 
    batch_size=64,
    num_workers=0
)

val_dataloader = validation_dataset.to_dataloader(
    train=False, 
    batch_size=128,           # 2x batch size per validation
    num_workers=0
)
```

---

## üìä Tabella Riepilogativa Ottimizzazioni

### Iperparametri Modello

| Parametro | Valore Iniziale | Valore Finale | Variazione | Motivazione |
|-----------|----------------|---------------|------------|-------------|
| `learning_rate` | 0.03 | **0.001** | **-97%** | Ridurre oscillazioni, convergenza stabile |
| `hidden_size` | 128 | **64** | **-50%** | Ridurre overfitting, semplificare |
| `lstm_layers` | 2 | **1** | **-50%** | Dataset piccolo, meno parametri |
| `hidden_continuous_size` | 16 | **8** | **-50%** | Proporzionale a hidden_size |
| `dropout` | 0.1 | **0.2** | **+100%** | Migliore regolarizzazione |
| `attention_head_size` | 4 | **4** | = | Adeguato per complessit√† |
| `output_size` | 7 | **7** | = | 7 quantili standard |

**Parametri totali:** ~450k ‚Üí ~120k (-73%) ‚úì

### Iperparametri Training

| Parametro | Valore Iniziale | Valore Finale | Variazione | Motivazione |
|-----------|----------------|---------------|------------|-------------|
| `max_epochs` | 50 | **100** | **+100%** | Pi√π tempo per convergenza con LR basso |
| `patience` | 10 | **20** | **+100%** | Evitare stop prematuro |
| `gradient_clip_val` | 0.1 | **1.0** | **+900%** | Permettere flusso gradiente |
| `batch_size` (train) | 64 | **64** | = | Bilanciato per GPU 8GB |
| `batch_size` (val) | 64 | **128** | **+100%** | Pi√π veloce, no backprop |

### Normalizzazione e Preprocessing

| Componente | Configurazione Iniziale | Configurazione Finale | Motivazione |
|------------|------------------------|----------------------|-------------|
| Target normalizer | `softplus` | **None** | Gestire meglio zeri (40% dati) |
| Missing values | Interpolazione | Interpolazione | Efficace, nessun cambio |
| Feature scaling | float32 | float32 | Ottimale per GPU |
| Time index | int | int | Requirement TFT |

---

## üîç Tracciamento Modifiche Dettagliato

### Commit 1: Setup Iniziale
**File:** `PV_Forecasting_TFT.ipynb`  
**Data:** Novembre 2025  
**Modifiche:**
- Creazione notebook con 13 sezioni
- Import librerie (PyTorch, Lightning, pytorch-forecasting)
- Data loading da Excel ‚Üí CSV (4 file)
- Data merging e preprocessing
- Feature engineering (temporali + meteo)
- TimeSeriesDataSet configuration
- Modello TFT iniziale
- Training e evaluation pipeline

### Commit 2: Fix Tecnici (8 problemi risolti)
**Riferimento:** `CHANGELOG_AND_FIXES.md`

**Problema 1 - Excel to CSV:**
```python
# Conversione 2 fogli Excel ‚Üí 4 file CSV
def excel_to_csv(excel_path, base_name):
    xl_file = pd.ExcelFile(excel_path)
    # Salva ogni foglio come CSV separato
```

**Problema 2 - Mixed Timestamp Formats:**
```python
# Gestione formati con/senza microsecondi
pd.to_datetime(data['datetime'], format='mixed', utc=True)
```

**Problema 3 - time_idx Type:**
```python
# DEVE essere intero, non float
data['time_idx'] = np.arange(len(data)).astype(int)
```

**Problema 4 - Lightning 2.x Namespace:**
```python
# PRIMA: import pytorch_lightning as pl
# DOPO: import lightning.pytorch as pl
```

**Problema 5 - Prediction Shape (Complesso):**
```python
# Gestione 3 casi possibili:
# 1. (batch, time) - solo mediana
# 2. (time, quantiles) - singola batch
# 3. (batch, time, quantiles) - completo

if len(pred_output.shape) == 2 and pred_output.shape[1] == 24:
    # Caso 1
elif len(pred_output.shape) == 2 and pred_output.shape[1] == 7:
    # Caso 2
elif len(pred_output.shape) == 3:
    # Caso 3
```

**Problema 6 - Single Sequence Visualization:**
```python
# Adatta plot al numero sequenze disponibili
num_sequences = min(5, len(y_pred))
if num_sequences == 1:
    # Un solo plot grande
else:
    # Subplot multipli
```

**Problema 7 - Interpretability Tuple:**
```python
# raw_predictions √® tupla
if isinstance(raw_predictions, tuple):
    raw_output = raw_predictions[0]
```

**Problema 8 - JSON Float32:**
```python
# NumPy float32 non serializzabile in JSON
results = {
    'MAE': float(mae),  # Conversione esplicita
    'RMSE': float(rmse),
    # ...
}
```

### Commit 3: Ottimizzazioni Modello
**Data:** Post-analisi "modello non apprende"  
**File modificati:** 
- `PV_Forecasting_TFT.ipynb` (celle 20, 23, 24, 25)
- Aggiunta cella diagnostica (5.1)

**Modifiche specifiche:**

**Cella 20 - TimeSeriesDataSet:**
```python
# Linea 15
- transformation="softplus"
+ transformation=None  # Cambiato per gestire zeri
```

**Cella 23 - Modello TFT:**
```python
# Linee 4-8
- learning_rate=0.03,
- hidden_size=128,
- lstm_layers=2,
- dropout=0.1,
- hidden_continuous_size=16,

+ learning_rate=0.001,  # Ridotto per stabilit√†
+ hidden_size=64,       # Semplificato
+ lstm_layers=1,        # Ridotto layers
+ dropout=0.2,          # Aumentato regolarizzazione
+ hidden_continuous_size=8,  # Proporzionale
```

**Cella 24 - Early Stopping:**
```python
# Linea 4
- patience=10,
+ patience=20,  # Pi√π paziente
```

**Cella 25 - Trainer:**
```python
# Linee 2-4
- max_epochs=50,
- gradient_clip_val=0.1,

+ max_epochs=100,         # Raddoppiato
+ gradient_clip_val=1.0,  # Meno aggressivo
```

**Nuova Cella 5.1 - Diagnostico:**
```python
# Inserita dopo cella 19 (split train/val)
# ~60 linee di codice per:
# - Statistiche globali
# - Conteggio zeri/non-zeri
# - Distribuzione oraria (ASCII chart)
# - Confronto train vs validation
# - Plot istogramma + boxplot
```

### Commit 4: Setup GPU
**Data:** Dopo identificazione PyTorch CPU-only  
**Modifiche sistema:**

```bash
# Terminal commands eseguiti
pip uninstall torch torchvision torchaudio -y
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install "numpy>=1.24,<2.3" --force-reinstall
```

**Notebook:**
- Aggiunta cella 3 "Verifica GPU/CUDA"
- Test GPU con tensor creato su cuda:0
- Output conferma: RTX 4060, 8GB, CUDA 11.8

---

## üìà Risultati Attesi vs Ottimizzazioni

### Prima delle Ottimizzazioni (Problematico)

```
Training:
  - Loss: 2.56 ‚Üí 2.56 ‚Üí 2.56 (costante)
  - Epochs completed: 10 (early stop)
  - Training time: N/A (stopped)
  
Predictions:
  - y_pred: [0, 0, 0, 0, 0, ..., 0] (tutti zeri)
  
Metrics:
  - MAE: molto alto
  - RMSE: molto alto
  - R¬≤: negativo o ~0
  - MAPE: >100%
```

### Dopo Ottimizzazioni (Atteso)

```
Training:
  - Loss: 2.56 ‚Üí 1.98 ‚Üí 1.45 ‚Üí ... ‚Üí 0.82 (decresce)
  - Epochs completed: 30-60 (convergenza vera)
  - Training time: ~10-15 min su GPU
  
Predictions:
  - y_pred: valori realistici che seguono pattern giorno/notte
  - Curve simili a y_true
  
Metrics:
  - MAE: 3-8 kW (ragionevole per 0-58 kW range)
  - RMSE: 5-12 kW
  - R¬≤: 0.6-0.85 (buono)
  - MAPE: 15-30% (accettabile per solar forecast)
```

---

## üéì Lezioni Apprese

### 1. Learning Rate √® Critico
- **Troppo alto** (0.03): modello oscilla, non converge
- **Ottimale** (0.001): convergenza stabile e graduale
- **Regola empirica**: iniziare con 0.001-0.01 per time series

### 2. Gradient Clipping Bilancia Delicato
- **Troppo aggressivo** (0.1): gradiente "scompare"
- **Troppo permissivo** (>5.0): rischio esplosione gradiente
- **Sweet spot** (1.0): permette flusso senza esplosione

### 3. Normalizzazione Dipende da Dati
- **Softplus**: buono per dati sempre positivi e continui
- **None (standard)**: migliore quando molti zeri presenti
- **Sempre analizzare distribuzione target prima di scegliere**

### 4. Complessit√† Modello vs Dataset Size
```
Dataset size: 17k samples
Sequence length: 168h encoder + 24h decoder = 192 timesteps
Features: 13 (8 meteo + 4 temporali + 1 target)

Modello troppo complesso:
  - 450k parametri
  - hidden=128, layers=2
  ‚Üí OVERFITTING

Modello appropriato:
  - 120k parametri (-73%)
  - hidden=64, layers=1
  ‚Üí GENERALIZZA MEGLIO
```

### 5. Early Stopping e Learning Rate
- **LR alto + patience basso**: stop prematuro prima di convergenza
- **LR basso + patience alto**: permette convergenza lenta ma stabile
- **Relazione**: patience ‚àù 1/learning_rate

### 6. GPU Setup per Python 3.13
- PyTorch CUDA 12.1 NON supporta Python 3.13 (Novembre 2025)
- PyTorch CUDA 11.8 S√å supporta Python 3.13 ‚úì
- Sempre verificare matrice compatibilit√† su pytorch.org

### 7. Diagnostic-Driven Development
- **Cella diagnostica salv√≤ il progetto**
- Identific√≤: 40% zeri, distribuzione bimodale, softplus problematico
- **Best practice**: sempre analizzare dati prima di training complesso

---

## üìö Riferimenti e Risorse

### Paper Originale TFT
Lim, B., et al. (2021). "Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting"  
https://arxiv.org/abs/1912.09363

### Librerie Utilizzate
- PyTorch 2.7.1+cu118
- PyTorch Lightning 2.5.6
- pytorch-forecasting 1.5.0
- pandas 2.3.3
- numpy 2.2.6
- scikit-learn 1.7.2

### Tools
- VS Code + Jupyter Extension
- CUDA 11.8
- TensorBoard (logging)
- Git (version control)

---

## üîú Prossimi Passi

### Immediate
1. ‚úÖ GPU configurata
2. ‚è≥ **Eseguire training completo con parametri ottimizzati**
3. ‚è≥ Valutare metriche finali
4. ‚è≥ Analizzare interpretabilit√† (attention weights)

### Miglioramenti Futuri
- [ ] Sperimentare encoder length (96h vs 168h vs 336h)
- [ ] Provare altre loss (Huber, MSE, Combined)
- [ ] Feature engineering avanzato (lag features, rolling statistics)
- [ ] Ensemble con altri modelli (LSTM, Prophet)
- [ ] Deploy modello (API REST, containerizzazione)
- [ ] Monitoraggio production (data drift, performance decay)

---

## ‚úÖ Checklist Finale

### Configurazione
- [x] Dataset caricato e preprocessato
- [x] Features engineering completato
- [x] TimeSeriesDataSet configurato
- [x] Train/Validation split corretto
- [x] DataLoaders creati

### Ottimizzazioni
- [x] Learning rate ottimizzato (0.03 ‚Üí 0.001)
- [x] Architettura semplificata (128‚Üí64, 2‚Üí1 layers)
- [x] Dropout aumentato (0.1 ‚Üí 0.2)
- [x] Gradient clipping rilassato (0.1 ‚Üí 1.0)
- [x] Normalizzazione corretta (softplus ‚Üí None)
- [x] Training prolungato (50 ‚Üí 100 epochs, patience 10 ‚Üí 20)
- [x] Cella diagnostica aggiunta

### Infrastructure
- [x] GPU configurata (RTX 4060, CUDA 11.8)
- [x] PyTorch GPU-enabled installato
- [x] Kernel riavviato con nuova config
- [x] Test GPU riuscito

### Documentazione
- [x] CHANGELOG_AND_FIXES.md creato
- [x] DEVELOPMENT_LOG.md creato (questo file)
- [x] Commit messages descrittivi
- [x] Code comments aggiornati

---

## üìù Note Conclusive

Questo documento traccia l'intera evoluzione del progetto da un modello non funzionante a una configurazione ottimizzata pronta per training su GPU. Le modifiche sono state guidate da:

1. **Analisi diagnostica dei dati** (40% zeri, distribuzione bimodale)
2. **Comprensione problema** (loss costante, predizioni zero)
3. **Ottimizzazioni teoriche** (learning rate, architettura, normalizzazione)
4. **Setup infrastructure** (GPU con CUDA 11.8)

Il progetto dimostra l'importanza di:
- üîç **Analisi esplorativa** prima del modeling
- üéØ **Hyperparameter tuning** informato
- üìä **Diagnostic-driven development**
- ‚ö° **Infrastructure appropriata** (GPU)
- üìù **Documentazione completa** del processo

---

**Versione:** 1.0  
**Ultimo aggiornamento:** Novembre 2025  
**Stato:** ‚úÖ Pronto per training finale

---
