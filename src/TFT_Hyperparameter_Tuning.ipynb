{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da4af341",
   "metadata": {},
   "source": [
    "# TFT Hyperparameter Tuning - Previsione Fotovoltaica 24h\n",
    "\n",
    "## Obiettivo\n",
    "Ottimizzazione degli iperparametri del Temporal Fusion Transformer (TFT) per la previsione della produzione fotovoltaica a 24 ore utilizzando:\n",
    "\n",
    "- **Dataset completo (100%)** con temporal cross-validation 80/20\n",
    "- **Ottimizzazione Bayesiana** tramite Optuna\n",
    "- **Salvataggio automatico** dei migliori iperparametri\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline Hyperparameter Tuning\n",
    "\n",
    "1. **Data Loading & Full Dataset Preparation**\n",
    "2. **Temporal Cross-Validation Setup** (80% train / 20% validation)\n",
    "3. **Hyperparameter Search Configuration** (Optuna)\n",
    "4. **Training & Validation Loop** con Early Stopping\n",
    "5. **Results Analysis & Best Model Selection**\n",
    "6. **Export Best Hyperparameters** to .txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4453fe8",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import di tutte le librerie necessarie per hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ae9f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  optuna-integration non trovato. Installazione in corso...\n",
      "======================================================================\n",
      "CONFIGURAZIONE SISTEMA\n",
      "======================================================================\n",
      "PyTorch version: 2.7.1+cu118\n",
      "PyTorch Lightning version: 2.6.0\n",
      "Optuna version: 4.6.0\n",
      "Device: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "======================================================================\n",
      "CUDA Memory: 8.00 GB\n",
      "======================================================================\n",
      "CONFIGURAZIONE SISTEMA\n",
      "======================================================================\n",
      "PyTorch version: 2.7.1+cu118\n",
      "PyTorch Lightning version: 2.6.0\n",
      "Optuna version: 4.6.0\n",
      "Device: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "======================================================================\n",
      "CUDA Memory: 8.00 GB\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation & base libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import json\n",
    "import time\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# PyTorch & PyTorch Lightning\n",
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "# PyTorch Forecasting\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, QuantileLoss\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "\n",
    "# Metrics & Evaluation\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "import optuna\n",
    "try:\n",
    "    from optuna.integration import PyTorchLightningPruningCallback\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  optuna-integration non trovato. Installazione in corso...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"optuna-integration[pytorch_lightning]\"])\n",
    "    from optuna.integration import PyTorchLightningPruningCallback\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIGURAZIONE SISTEMA\")\n",
    "print(\"=\"*70)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PyTorch Lightning version: {pl.__version__}\")\n",
    "print(f\"Optuna version: {optuna.__version__}\")\n",
    "\n",
    "print(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d86add9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CONTROLLO E INSTALLAZIONE DIPENDENZE\n",
      "============================================================\n",
      "‚úì optuna>=3.4.0 gi√† installato\n",
      "üì¶ Installazione optuna-integration[pytorch_lightning]...\n",
      "‚úÖ optuna-integration[pytorch_lightning] installato con successo\n",
      "\n",
      "‚úÖ Tutte le dipendenze sono installate correttamente!\n",
      "============================================================\n",
      "‚úÖ optuna-integration[pytorch_lightning] installato con successo\n",
      "\n",
      "‚úÖ Tutte le dipendenze sono installate correttamente!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Installazione dipendenze mancanti (eseguire solo se necessario)\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Installa un pacchetto usando pip\"\"\"\n",
    "    try:\n",
    "        import importlib\n",
    "        importlib.import_module(package.split('>=')[0].split('==')[0])\n",
    "        print(f\"‚úì {package} gi√† installato\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(f\"üì¶ Installazione {package}...\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "            print(f\"‚úÖ {package} installato con successo\")\n",
    "            return True\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"‚ùå Errore nell'installazione di {package}\")\n",
    "            return False\n",
    "\n",
    "# Lista dei pacchetti necessari per hyperparameter tuning\n",
    "required_packages = [\n",
    "    \"optuna>=3.4.0\",\n",
    "    \"optuna-integration[pytorch_lightning]\"\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CONTROLLO E INSTALLAZIONE DIPENDENZE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_installed = True\n",
    "for package in required_packages:\n",
    "    if not install_package(package):\n",
    "        all_installed = False\n",
    "\n",
    "if all_installed:\n",
    "    print(\"\\n‚úÖ Tutte le dipendenze sono installate correttamente!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Alcune dipendenze potrebbero non essere state installate.\")\n",
    "    print(\"Se necessario, esegui manualmente: pip install optuna\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d141ad",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Full Dataset Preparation\n",
    "\n",
    "Caricamento del dataset completo (100%) e preparazione per hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "991bb368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CARICAMENTO DATASET COMPLETO\n",
      "======================================================================\n",
      "‚úì Trovato: pv_dataset - 07-10--06-11.csv\n",
      "‚úì Trovato: pv_dataset - 07-11--06-12.csv\n",
      "‚úì Trovato: wx_dataset - 07-10--06-11.csv\n",
      "‚úì Trovato: wx_dataset - 07-11--06-12.csv\n",
      "\n",
      "Caricamento dati PV...\n",
      "\n",
      "Caricamento dati meteo...\n",
      "\n",
      "Processing timestamps...\n",
      "\n",
      "Dataset merged shape: (17317, 16)\n",
      "Range temporale: 2010-07-01 00:00:00 to 2012-06-30 13:00:00\n"
     ]
    }
   ],
   "source": [
    "def load_full_dataset():\n",
    "    \"\"\"\n",
    "    Carica e prepara il dataset completo PV + meteo\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataset completo processato\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"CARICAMENTO DATASET COMPLETO\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Trova la directory root del progetto\n",
    "    project_root = Path('..').resolve()\n",
    "    \n",
    "    # Funzione per trovare file CSV\n",
    "    def find_csv_file(filename):\n",
    "        possible_paths = [\n",
    "            project_root / 'data' / 'raw' / filename,\n",
    "            project_root / 'data' / filename,\n",
    "            project_root / filename,\n",
    "            Path('.') / filename,\n",
    "            Path('..') / filename\n",
    "        ]\n",
    "        \n",
    "        for path in possible_paths:\n",
    "            if path.exists():\n",
    "                print(f\"‚úì Trovato: {filename}\")\n",
    "                return str(path)\n",
    "        \n",
    "        print(f\"‚úó Non trovato: {filename}\")\n",
    "        return None\n",
    "    \n",
    "    # Trova e carica i file CSV\n",
    "    pv_file1 = find_csv_file(\"pv_dataset - 07-10--06-11.csv\")\n",
    "    pv_file2 = find_csv_file(\"pv_dataset - 07-11--06-12.csv\")\n",
    "    wx_file1 = find_csv_file(\"wx_dataset - 07-10--06-11.csv\")\n",
    "    wx_file2 = find_csv_file(\"wx_dataset - 07-11--06-12.csv\")\n",
    "    \n",
    "    if not all([pv_file1, pv_file2, wx_file1, wx_file2]):\n",
    "        raise FileNotFoundError(\"Non tutti i file CSV sono stati trovati!\")\n",
    "    \n",
    "    print(\"\\nCaricamento dati PV...\")\n",
    "    pv1 = pd.read_csv(pv_file1)\n",
    "    pv2 = pd.read_csv(pv_file2)\n",
    "    pv_data = pd.concat([pv1, pv2], ignore_index=True)\n",
    "    \n",
    "    # Rinomina colonne PV\n",
    "    timestamp_col = \"Max kWp\"\n",
    "    target_col = [col for col in pv_data.columns if col != timestamp_col][0]\n",
    "    pv_data = pv_data.rename(columns={timestamp_col: 'datetime', target_col: 'power_kw'})\n",
    "    \n",
    "    print(\"\\nCaricamento dati meteo...\")\n",
    "    wx1 = pd.read_csv(wx_file1)\n",
    "    wx2 = pd.read_csv(wx_file2)\n",
    "    wx_data = pd.concat([wx1, wx2], ignore_index=True)\n",
    "    wx_data = wx_data.rename(columns={'dt_iso': 'datetime'})\n",
    "    \n",
    "    print(\"\\nProcessing timestamps...\")\n",
    "    # Conversione timestamp\n",
    "    pv_data['datetime'] = pd.to_datetime(pv_data['datetime'], format='mixed', utc=True).dt.tz_localize(None)\n",
    "    wx_data['datetime'] = pd.to_datetime(wx_data['datetime'], format='mixed', utc=True).dt.tz_localize(None)\n",
    "    \n",
    "    # Rimuovi duplicati\n",
    "    pv_data = pv_data.drop_duplicates(subset=['datetime'], keep='first')\n",
    "    wx_data = wx_data.drop_duplicates(subset=['datetime'], keep='first')\n",
    "    \n",
    "    # Merge datasets\n",
    "    data = pd.merge(pv_data, wx_data, on='datetime', how='inner')\n",
    "    data = data.sort_values('datetime').reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nDataset merged shape: {data.shape}\")\n",
    "    print(f\"Range temporale: {data['datetime'].min()} to {data['datetime'].max()}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Carica dataset\n",
    "full_data = load_full_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9ae0023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PREPROCESSING DATA\n",
      "======================================================================\n",
      "Gestione valori mancanti...\n",
      "‚úì rain_1h: NaN sostituiti con 0\n",
      "Righe rimosse: 0\n",
      "\n",
      "Feature Engineering...\n",
      "‚úì Dataset processato: (17317, 22)\n",
      "‚úì Feature temporali: hour, day_of_month, month, day_of_week\n",
      "‚úì group_id e time_idx creati\n",
      "\n",
      "Dataset finale: (17317, 22)\n",
      "Colonne: ['datetime', 'power_kw', 'lat', 'lon', 'temp', 'dew_point', 'pressure', 'humidity', 'wind_speed', 'wind_deg', 'rain_1h', 'clouds_all', 'weather_description', 'Dhi', 'Dni', 'Ghi', 'hour', 'day_of_month', 'month', 'day_of_week', 'group_id', 'time_idx']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    Preprocessing completo del dataset: missing values, feature engineering\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PREPROCESSING DATA\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Gestione valori mancanti\n",
    "    print(\"Gestione valori mancanti...\")\n",
    "    \n",
    "    # rain_1h: NaN = 0 (nessuna pioggia)\n",
    "    if 'rain_1h' in data.columns:\n",
    "        data['rain_1h'] = data['rain_1h'].fillna(0)\n",
    "        print(\"‚úì rain_1h: NaN sostituiti con 0\")\n",
    "    \n",
    "    # Interpolazione per altre colonne numeriche\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    for col in numeric_cols:\n",
    "        if col != 'power_kw' and data[col].isnull().sum() > 0:\n",
    "            data[col] = data[col].interpolate(method='linear', limit=5)\n",
    "            data[col] = data[col].fillna(method='ffill', limit=2)\n",
    "    \n",
    "    # Gestione power_kw\n",
    "    if 'power_kw' in numeric_cols:\n",
    "        data['power_kw'] = data['power_kw'].interpolate(method='linear', limit=3)\n",
    "        data['power_kw'] = data['power_kw'].fillna(method='ffill', limit=1)\n",
    "    \n",
    "    # Rimuovi righe con NaN rimanenti\n",
    "    rows_before = len(data)\n",
    "    data = data.dropna()\n",
    "    print(f\"Righe rimosse: {rows_before - len(data)}\")\n",
    "    \n",
    "    # Feature Engineering\n",
    "    print(\"\\nFeature Engineering...\")\n",
    "    \n",
    "    # Feature temporali\n",
    "    data['hour'] = data['datetime'].dt.hour\n",
    "    data['day_of_month'] = data['datetime'].dt.day\n",
    "    data['month'] = data['datetime'].dt.month\n",
    "    data['day_of_week'] = data['datetime'].dt.dayofweek\n",
    "    \n",
    "    # Group ID e time index per TFT\n",
    "    data['group_id'] = 'PV1'\n",
    "    data['time_idx'] = np.arange(len(data)).astype(int)\n",
    "    \n",
    "    # Conversione tipi\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    for col in numeric_cols:\n",
    "        if col != 'time_idx':\n",
    "            data[col] = data[col].astype(np.float32)\n",
    "    \n",
    "    print(f\"‚úì Dataset processato: {data.shape}\")\n",
    "    print(f\"‚úì Feature temporali: hour, day_of_month, month, day_of_week\")\n",
    "    print(f\"‚úì group_id e time_idx creati\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Preprocessing completo\n",
    "processed_data = preprocess_data(full_data.copy())\n",
    "\n",
    "print(f\"\\nDataset finale: {processed_data.shape}\")\n",
    "print(f\"Colonne: {processed_data.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd6786d",
   "metadata": {},
   "source": [
    "## 3. Temporal Cross-Validation Setup (80/20)\n",
    "\n",
    "Configurazione della temporal cross-validation per evitare data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3833d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEMPORAL CROSS-VALIDATION SETUP (BALANCED FOLDS)\n",
      "======================================================================\n",
      "Dataset totale: 17317 samples\n",
      "Validation size fisso: 3463 samples (20.0%)\n",
      "Min training per fold: 13854 samples\n",
      "\n",
      "Fold 1/5:\n",
      "  Training: 8658 samples (50.0%)\n",
      "  Validation: 3463 samples (20.0%)\n",
      "  Train range: 2010-07-01 00:00:00 to 2011-07-05 18:00:00\n",
      "  Val range: 2011-07-05 19:00:00 to 2011-11-27 01:00:00\n",
      "  Gap check: 0 hours between train/val\n",
      "\n",
      "Fold 2/5:\n",
      "  Training: 9957 samples (57.5%)\n",
      "  Validation: 3463 samples (20.0%)\n",
      "  Train range: 2010-07-01 00:00:00 to 2011-08-28 21:00:00\n",
      "  Val range: 2011-08-28 22:00:00 to 2012-01-20 04:00:00\n",
      "  Gap check: 0 hours between train/val\n",
      "\n",
      "Fold 3/5:\n",
      "  Training: 11256 samples (65.0%)\n",
      "  Validation: 3463 samples (20.0%)\n",
      "  Train range: 2010-07-01 00:00:00 to 2011-10-22 00:00:00\n",
      "  Val range: 2011-10-22 01:00:00 to 2012-03-14 07:00:00\n",
      "  Gap check: 0 hours between train/val\n",
      "\n",
      "Fold 4/5:\n",
      "  Training: 12555 samples (72.5%)\n",
      "  Validation: 3463 samples (20.0%)\n",
      "  Train range: 2010-07-01 00:00:00 to 2011-12-15 03:00:00\n",
      "  Val range: 2011-12-15 04:00:00 to 2012-05-07 10:00:00\n",
      "  Gap check: 0 hours between train/val\n",
      "\n",
      "Fold 5/5:\n",
      "  Training: 13854 samples (80.0%)\n",
      "  Validation: 3463 samples (20.0%)\n",
      "  Train range: 2010-07-01 00:00:00 to 2012-02-07 06:00:00\n",
      "  Val range: 2012-02-07 07:00:00 to 2012-06-30 13:00:00\n",
      "  Gap check: 0 hours between train/val\n",
      "\n",
      "‚úì Creati 5 fold temporali validi con validation set bilanciato\n",
      "\n",
      "üìä Cross-validation configurata con 5 fold\n",
      "Ogni trial verr√† validato su tutti i fold per robustezza\n",
      "\n",
      "Feature identificate:\n",
      "  - Static categoricals: ['group_id']\n",
      "  - Unknown reals: ['power_kw']\n",
      "  - Known reals total: 12\n",
      "  - Weather features: ['temp', 'Dni', 'Ghi', 'humidity', 'clouds_all', 'wind_speed', 'pressure', 'rain_1h']\n",
      "  - Time features: ['hour', 'day_of_month', 'month', 'day_of_week']\n"
     ]
    }
   ],
   "source": [
    "# Configurazione TFT - Parametri fissi\n",
    "MAX_ENCODER_LENGTH = 168  # 1 settimana di contesto\n",
    "MAX_PREDICTION_LENGTH = 24  # Previsione 24 ore\n",
    "\n",
    "def setup_temporal_cross_validation(data, n_folds=5, val_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Temporal Cross-Validation con fold bilanciati per evitare data leakage\n",
    "    \n",
    "    Args:\n",
    "        data: Dataset completo\n",
    "        n_folds: Numero di fold temporali\n",
    "        val_ratio: Percentuale fissa per validation set (es. 0.2 = 20%)\n",
    "    \n",
    "    Returns:\n",
    "        list: Lista di tuple (train_data, val_data) per ogni fold\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"TEMPORAL CROSS-VALIDATION SETUP (BALANCED FOLDS)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    max_time_idx = data['time_idx'].max()\n",
    "    total_samples = len(data)\n",
    "    val_size = int(total_samples * val_ratio)  # Validation set fisso (20%)\n",
    "    folds = []\n",
    "    \n",
    "    print(f\"Dataset totale: {total_samples} samples\")\n",
    "    print(f\"Validation size fisso: {val_size} samples ({val_ratio*100:.1f}%)\")\n",
    "    print(f\"Min training per fold: {total_samples - val_size} samples\")\n",
    "    \n",
    "    # Calcola la dimensione incrementale per il training set\n",
    "    min_train_size = int(total_samples * 0.5)  # Minimo 50% per training\n",
    "    max_train_size = total_samples - val_size  # Massimo possibile\n",
    "    \n",
    "    for fold in range(n_folds):\n",
    "        # Training size cresce progressivamente\n",
    "        progress = fold / (n_folds - 1)  # 0.0 a 1.0\n",
    "        train_size = int(min_train_size + progress * (max_train_size - min_train_size))\n",
    "        \n",
    "        # Assicura che train + val non superi il dataset\n",
    "        if train_size + val_size > total_samples:\n",
    "            train_size = total_samples - val_size\n",
    "            \n",
    "        # Calcola cutoff temporali\n",
    "        train_cutoff = train_size - 1  # time_idx da 0 a train_cutoff\n",
    "        val_start = train_cutoff + 1\n",
    "        val_cutoff = val_start + val_size - 1\n",
    "        \n",
    "        # Assicura che non superi i limiti del dataset\n",
    "        if val_cutoff >= max_time_idx:\n",
    "            val_cutoff = max_time_idx\n",
    "            val_start = val_cutoff - val_size + 1\n",
    "            train_cutoff = val_start - 1\n",
    "            \n",
    "        # Skip se training set troppo piccolo\n",
    "        if train_cutoff < MAX_ENCODER_LENGTH + MAX_PREDICTION_LENGTH:\n",
    "            print(f\"Skipping fold {fold + 1}: training set troppo piccolo\")\n",
    "            continue\n",
    "            \n",
    "        # Skip se validation set troppo piccolo\n",
    "        if (val_cutoff - val_start + 1) < MAX_PREDICTION_LENGTH * 5:\n",
    "            print(f\"Skipping fold {fold + 1}: validation set troppo piccolo\")\n",
    "            continue\n",
    "            \n",
    "        train_data = data[data['time_idx'] <= train_cutoff].copy()\n",
    "        val_data = data[(data['time_idx'] >= val_start) & (data['time_idx'] <= val_cutoff)].copy()\n",
    "        \n",
    "        folds.append((train_data, val_data))\n",
    "        \n",
    "        print(f\"\\nFold {fold + 1}/{n_folds}:\")\n",
    "        print(f\"  Training: {len(train_data)} samples ({len(train_data)/total_samples*100:.1f}%)\")\n",
    "        print(f\"  Validation: {len(val_data)} samples ({len(val_data)/total_samples*100:.1f}%)\")\n",
    "        print(f\"  Train range: {train_data['datetime'].min()} to {train_data['datetime'].max()}\")\n",
    "        print(f\"  Val range: {val_data['datetime'].min()} to {val_data['datetime'].max()}\")\n",
    "        print(f\"  Gap check: {val_data['time_idx'].min() - train_data['time_idx'].max() - 1} hours between train/val\")\n",
    "    \n",
    "    print(f\"\\n‚úì Creati {len(folds)} fold temporali validi con validation set bilanciato\")\n",
    "    return folds\n",
    "\n",
    "# Setup temporal cross-validation con fold bilanciati\n",
    "folds = setup_temporal_cross_validation(processed_data, n_folds=5, val_ratio=0.2)\n",
    "\n",
    "def identify_features(data):\n",
    "    \"\"\"\n",
    "    Identifica le feature per TFT\n",
    "    \"\"\"\n",
    "    # Colonne meteo disponibili\n",
    "    weather_cols = []\n",
    "    for col in ['temp', 'Dni', 'Ghi', 'humidity', 'clouds_all', 'wind_speed', 'pressure', 'rain_1h']:\n",
    "        if col in data.columns:\n",
    "            weather_cols.append(col)\n",
    "    \n",
    "    # Feature temporali\n",
    "    time_features = ['hour', 'day_of_month', 'month', 'day_of_week']\n",
    "    \n",
    "    # Known reals (note nel futuro)\n",
    "    known_reals = time_features + weather_cols\n",
    "    return known_reals\n",
    "\n",
    "known_reals = identify_features(processed_data)\n",
    "\n",
    "print(f\"\\nüìä Cross-validation configurata con {len(folds)} fold\")\n",
    "print(\"Ogni trial verr√† validato su tutti i fold per robustezza\")\n",
    "print(f\"\\nFeature identificate:\")\n",
    "print(f\"  - Static categoricals: ['group_id']\")\n",
    "print(f\"  - Unknown reals: ['power_kw']\")\n",
    "print(f\"  - Known reals total: {len(known_reals)}\")\n",
    "print(f\"  - Weather features: {[col for col in known_reals if col not in ['hour', 'day_of_month', 'month', 'day_of_week']]}\")\n",
    "print(f\"  - Time features: {['hour', 'day_of_month', 'month', 'day_of_week']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b2db4",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Search Configuration (Optuna)\n",
    "\n",
    "Definizione dello spazio di ricerca per gli iperparametri del TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a159e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CONFIGURAZIONE HYPERPARAMETER SEARCH\n",
      "======================================================================\n",
      "Spazio di ricerca definito:\n",
      "  - hidden_size: [64, 128, 192, 256]\n",
      "  - lstm_layers: [1, 2, 3]\n",
      "  - attention_head_size: [1, 2, 4, 8]\n",
      "  - hidden_continuous_size: [8, 16, 32]\n",
      "  - dropout: [0.1, 0.4]\n",
      "  - learning_rate: [1e-4, 1e-1] (log)\n",
      "  - batch_size: [32, 64, 128]\n",
      "  - patience: [10, 30]\n",
      "  - gradient_clip_val: [0.1, 2.0]\n",
      "\n",
      "Algoritmo: TPE Sampler\n",
      "Pruning: Median Pruner\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def suggest_hyperparameters(trial):\n",
    "    \"\"\"\n",
    "    Definisce lo spazio di ricerca degli iperparametri per Optuna\n",
    "    \"\"\"\n",
    "    hyperparams = {\n",
    "        # Architettura del modello\n",
    "        'hidden_size': trial.suggest_categorical('hidden_size', [64, 128, 192, 256]),\n",
    "        'lstm_layers': trial.suggest_int('lstm_layers', 1, 3),\n",
    "        'attention_head_size': trial.suggest_categorical('attention_head_size', [1, 2, 4, 8]),\n",
    "        'hidden_continuous_size': trial.suggest_categorical('hidden_continuous_size', [8, 16, 32]),\n",
    "        \n",
    "        # Regolarizzazione\n",
    "        'dropout': trial.suggest_float('dropout', 0.1, 0.4),\n",
    "        \n",
    "        # Training parameters\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [32, 64, 128]),\n",
    "        \n",
    "        # Early stopping patience\n",
    "        'patience': trial.suggest_int('patience', 10, 30),\n",
    "        \n",
    "        # Gradient clipping\n",
    "        'gradient_clip_val': trial.suggest_float('gradient_clip_val', 0.1, 2.0),\n",
    "    }\n",
    "    \n",
    "    return hyperparams\n",
    "\n",
    "# Configurazione Optuna Study\n",
    "def create_optuna_study():\n",
    "    \"\"\"\n",
    "    Crea studio Optuna per l'ottimizzazione\n",
    "    \"\"\"\n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",  # Minimizza validation loss\n",
    "        study_name=\"tft_hyperparameter_tuning\",\n",
    "        sampler=optuna.samplers.TPESampler(seed=42),\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)\n",
    "    )\n",
    "    return study\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIGURAZIONE HYPERPARAMETER SEARCH\")\n",
    "print(\"=\"*70)\n",
    "print(\"Spazio di ricerca definito:\")\n",
    "print(\"  - hidden_size: [64, 128, 192, 256]\")\n",
    "print(\"  - lstm_layers: [1, 2, 3]\")\n",
    "print(\"  - attention_head_size: [1, 2, 4, 8]\")\n",
    "print(\"  - hidden_continuous_size: [8, 16, 32]\")\n",
    "print(\"  - dropout: [0.1, 0.4]\")\n",
    "print(\"  - learning_rate: [1e-4, 1e-1] (log)\")\n",
    "print(\"  - batch_size: [32, 64, 128]\")\n",
    "print(\"  - patience: [10, 30]\")\n",
    "print(\"  - gradient_clip_val: [0.1, 2.0]\")\n",
    "print(\"\\nAlgoritmo: TPE Sampler\")\n",
    "print(\"Pruning: Median Pruner\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787ee398",
   "metadata": {},
   "source": [
    "## 5. Training & Validation Loop\n",
    "\n",
    "Implementazione del training loop con Optuna per l'ottimizzazione degli iperparametri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "191f4418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Funzione obiettivo definita\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Funzione obiettivo per Optuna con temporal cross-validation\n",
    "    Valuta gli iperparametri su tutti i fold e restituisce la media\n",
    "    \"\"\"\n",
    "    # Suggest hyperparameters\n",
    "    hyperparams = suggest_hyperparameters(trial)\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TRIAL {trial.number} - TEMPORAL CROSS-VALIDATION\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(\"Hyperparameters:\")\n",
    "        for key, value in hyperparams.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        fold_losses = []\n",
    "        \n",
    "        # Loop attraverso tutti i fold per cross-validation\n",
    "        for fold_idx, (train_data, val_data) in enumerate(folds):\n",
    "            print(f\"\\n--- Training Fold {fold_idx + 1}/{len(folds)} ---\")\n",
    "            \n",
    "            # Crea TimeSeriesDataSet per questo fold\n",
    "            training_dataset = TimeSeriesDataSet(\n",
    "                train_data,\n",
    "                time_idx=\"time_idx\",\n",
    "                target=\"power_kw\",\n",
    "                group_ids=[\"group_id\"],\n",
    "                min_encoder_length=MAX_ENCODER_LENGTH,\n",
    "                max_encoder_length=MAX_ENCODER_LENGTH,\n",
    "                min_prediction_length=MAX_PREDICTION_LENGTH,\n",
    "                max_prediction_length=MAX_PREDICTION_LENGTH,\n",
    "                static_categoricals=[\"group_id\"],\n",
    "                time_varying_known_reals=known_reals,\n",
    "                time_varying_unknown_reals=[\"power_kw\"],\n",
    "                target_normalizer=GroupNormalizer(\n",
    "                    groups=[\"group_id\"], \n",
    "                    transformation=None\n",
    "                ),\n",
    "                add_relative_time_idx=True,\n",
    "                add_target_scales=True,\n",
    "                add_encoder_length=True,\n",
    "            )\n",
    "            \n",
    "            # Validation dataset per questo fold\n",
    "            fold_data = pd.concat([train_data, val_data]).sort_values('time_idx')\n",
    "            validation_dataset = TimeSeriesDataSet.from_dataset(\n",
    "                training_dataset, \n",
    "                fold_data,\n",
    "                predict=True, \n",
    "                stop_randomization=True\n",
    "            )\n",
    "            \n",
    "            # DataLoaders per questo fold\n",
    "            train_dataloader = training_dataset.to_dataloader(\n",
    "                train=True, \n",
    "                batch_size=hyperparams['batch_size'], \n",
    "                num_workers=0\n",
    "            )\n",
    "            \n",
    "            val_dataloader = validation_dataset.to_dataloader(\n",
    "                train=False, \n",
    "                batch_size=hyperparams['batch_size'] * 2, \n",
    "                num_workers=0\n",
    "            )\n",
    "            \n",
    "            # Crea modello TFT per questo fold\n",
    "            tft = TemporalFusionTransformer.from_dataset(\n",
    "                training_dataset,\n",
    "                learning_rate=hyperparams['learning_rate'],\n",
    "                hidden_size=hyperparams['hidden_size'],\n",
    "                lstm_layers=hyperparams['lstm_layers'],\n",
    "                attention_head_size=hyperparams['attention_head_size'],\n",
    "                dropout=hyperparams['dropout'],\n",
    "                hidden_continuous_size=hyperparams['hidden_continuous_size'],\n",
    "                output_size=7,\n",
    "                loss=QuantileLoss(),\n",
    "                log_interval=10,\n",
    "                reduce_on_plateau_patience=4,\n",
    "            )\n",
    "            \n",
    "            # Callbacks per questo fold\n",
    "            early_stop_callback = EarlyStopping(\n",
    "                monitor=\"val_loss\",\n",
    "                min_delta=1e-4,\n",
    "                patience=max(5, hyperparams['patience'] // 2),  # Patience ridotta per fold\n",
    "                verbose=False,\n",
    "                mode=\"min\"\n",
    "            )\n",
    "            \n",
    "            # Logger per fold specifico\n",
    "            logger = TensorBoardLogger(\n",
    "                \"lightning_logs\", \n",
    "                name=f\"trial_{trial.number}_fold_{fold_idx}\"\n",
    "            )\n",
    "            \n",
    "            # Trainer per questo fold\n",
    "            trainer = pl.Trainer(\n",
    "                max_epochs=30,  # Epochs ridotte per fold multiple\n",
    "                accelerator=\"auto\",\n",
    "                devices=1,\n",
    "                gradient_clip_val=hyperparams['gradient_clip_val'],\n",
    "                callbacks=[early_stop_callback],\n",
    "                logger=logger,\n",
    "                enable_progress_bar=False,\n",
    "                enable_model_summary=False,\n",
    "                enable_checkpointing=False,\n",
    "            )\n",
    "            \n",
    "            # Training su questo fold\n",
    "            trainer.fit(\n",
    "                tft,\n",
    "                train_dataloaders=train_dataloader,\n",
    "                val_dataloaders=val_dataloader,\n",
    "            )\n",
    "            \n",
    "            # Ottieni validation loss per questo fold\n",
    "            fold_val_loss = trainer.callback_metrics.get(\"val_loss\")\n",
    "            \n",
    "            if fold_val_loss is None:\n",
    "                fold_val_loss = float('inf')\n",
    "            else:\n",
    "                fold_val_loss = fold_val_loss.item()\n",
    "            \n",
    "            fold_losses.append(fold_val_loss)\n",
    "            print(f\"Fold {fold_idx + 1} - Validation Loss: {fold_val_loss:.6f}\")\n",
    "            \n",
    "            # Cleanup memoria per questo fold\n",
    "            del tft, trainer, training_dataset, validation_dataset\n",
    "            del train_dataloader, val_dataloader\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calcola media validation loss su tutti i fold\n",
    "        mean_val_loss = np.mean(fold_losses)\n",
    "        std_val_loss = np.std(fold_losses)\n",
    "        \n",
    "        print(f\"\\nüìä Trial {trial.number} RISULTATI:\")\n",
    "        print(f\"  Mean Val Loss: {mean_val_loss:.6f} (¬±{std_val_loss:.6f})\")\n",
    "        print(f\"  Fold losses: {[f'{loss:.6f}' for loss in fold_losses]}\")\n",
    "        \n",
    "        # Per Optuna pruning, usa la media\n",
    "        for fold_idx, loss in enumerate(fold_losses):\n",
    "            trial.report(loss, fold_idx)\n",
    "            if trial.should_prune():\n",
    "                print(f\"  ‚ö° Trial pruned at fold {fold_idx + 1}\")\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "        return mean_val_loss\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Trial {trial.number} FAILED: {str(e)}\")\n",
    "        # Cleanup in caso di errore\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        gc.collect()\n",
    "        return float('inf')\n",
    "\n",
    "print(\"‚úì Funzione obiettivo definita\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae26681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 19:53:04,894] A new study created in memory with name: tft_hyperparameter_tuning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "AVVIO HYPERPARAMETER TUNING\n",
      "======================================================================\n",
      "Numero di trials: 5\n",
      "Max epochs per trial: 50\n",
      "Early stopping patience: variabile (10-30)\n",
      "Algoritmo: TPE Sampler\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRIAL 0 - TEMPORAL CROSS-VALIDATION\n",
      "============================================================\n",
      "Hyperparameters:\n",
      "  hidden_size: 128\n",
      "  lstm_layers: 1\n",
      "  attention_head_size: 4\n",
      "  hidden_continuous_size: 32\n",
      "  dropout: 0.34973279224012654\n",
      "  learning_rate: 0.0004335281794951569\n",
      "  batch_size: 128\n",
      "  patience: 21\n",
      "  gradient_clip_val: 0.9206955354200199\n",
      "\n",
      "--- Training Fold 1/5 ---\n"
     ]
    }
   ],
   "source": [
    "# Configurazione e avvio hyperparameter tuning\n",
    "N_TRIALS = 5  # Numero di trial da eseguire (modificabile)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"AVVIO HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Numero di trials: {N_TRIALS}\")\n",
    "print(f\"Max epochs per trial: 50\")\n",
    "print(f\"Early stopping patience: variabile (10-30)\")\n",
    "print(f\"Algoritmo: TPE Sampler\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Crea studio Optuna\n",
    "study = create_optuna_study()\n",
    "\n",
    "# Avvia ottimizzazione\n",
    "start_time = time.time()\n",
    "\n",
    "study.optimize(\n",
    "    objective, \n",
    "    n_trials=N_TRIALS,\n",
    "    timeout=None,  # Nessun timeout\n",
    "    gc_after_trial=True,  # Garbage collection dopo ogni trial\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HYPERPARAMETER TUNING COMPLETATO\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Tempo totale: {duration/3600:.2f} ore\")\n",
    "print(f\"Trials completati: {len(study.trials)}\")\n",
    "print(f\"Best trial: {study.best_trial.number}\")\n",
    "print(f\"Best validation loss: {study.best_value:.6f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d06227",
   "metadata": {},
   "source": [
    "## 6. Results Collection & Analysis\n",
    "\n",
    "Analisi dei risultati dell'hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2facc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisi dei risultati\n",
    "print(\"=\"*70)\n",
    "print(\"ANALISI RISULTATI HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Best trial\n",
    "best_trial = study.best_trial\n",
    "print(f\"\\nBest Trial #{best_trial.number}\")\n",
    "print(f\"Best Validation Loss: {best_trial.value:.6f}\")\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Top 10 trials\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"TOP 10 TRIALS\")\n",
    "print(\"-\"*50)\n",
    "trials_df = study.trials_dataframe().sort_values('value')\n",
    "top_10 = trials_df.head(10)\n",
    "\n",
    "print(f\"{'Rank':<5} {'Trial':<6} {'Val Loss':<12} {'Duration':<10} {'State':<10}\")\n",
    "print(\"-\"*50)\n",
    "for i, (_, row) in enumerate(top_10.iterrows(), 1):\n",
    "    duration_sec = (row['datetime_complete'] - row['datetime_start']).total_seconds() if pd.notna(row['datetime_complete']) else 0\n",
    "    duration_min = duration_sec / 60\n",
    "    print(f\"{i:<5} {int(row['number']):<6} {row['value']:<12.6f} {duration_min:<10.1f} {row['state']:<10}\")\n",
    "\n",
    "# Statistiche generali\n",
    "completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "failed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.FAIL]\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "\n",
    "print(f\"\\n\" + \"-\"*50)\n",
    "print(\"STATISTICHE GENERALI\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Trials completati: {len(completed_trials)}\")\n",
    "print(f\"Trials falliti: {len(failed_trials)}\")\n",
    "print(f\"Trials pruned: {len(pruned_trials)}\")\n",
    "print(f\"Success rate: {len(completed_trials)/len(study.trials)*100:.1f}%\")\n",
    "\n",
    "if len(completed_trials) > 0:\n",
    "    val_losses = [t.value for t in completed_trials]\n",
    "    print(f\"\\nValidation Loss Statistics:\")\n",
    "    print(f\"  Min: {min(val_losses):.6f}\")\n",
    "    print(f\"  Max: {max(val_losses):.6f}\")\n",
    "    print(f\"  Mean: {np.mean(val_losses):.6f}\")\n",
    "    print(f\"  Std: {np.std(val_losses):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33306604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizzazione risultati\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Optimization History\n",
    "completed_trials_values = [(t.number, t.value) for t in completed_trials]\n",
    "if completed_trials_values:\n",
    "    trial_numbers, values = zip(*completed_trials_values)\n",
    "    axes[0, 0].plot(trial_numbers, values, 'bo-', alpha=0.6)\n",
    "    axes[0, 0].set_xlabel('Trial Number')\n",
    "    axes[0, 0].set_ylabel('Validation Loss')\n",
    "    axes[0, 0].set_title('Optimization History')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Parameter Importance (se disponibile)\n",
    "try:\n",
    "    importance = optuna.importance.get_param_importances(study)\n",
    "    if importance:\n",
    "        params, importances = zip(*importance.items())\n",
    "        axes[0, 1].barh(params, importances)\n",
    "        axes[0, 1].set_xlabel('Importance')\n",
    "        axes[0, 1].set_title('Parameter Importance')\n",
    "except:\n",
    "    axes[0, 1].text(0.5, 0.5, 'Parameter Importance\\nNot Available', \n",
    "                   ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "\n",
    "# 3. Learning Rate vs Validation Loss\n",
    "if completed_trials:\n",
    "    lr_values = [t.params.get('learning_rate', 0) for t in completed_trials]\n",
    "    val_losses = [t.value for t in completed_trials]\n",
    "    axes[1, 0].scatter(lr_values, val_losses, alpha=0.6)\n",
    "    axes[1, 0].set_xlabel('Learning Rate')\n",
    "    axes[1, 0].set_ylabel('Validation Loss')\n",
    "    axes[1, 0].set_title('Learning Rate vs Validation Loss')\n",
    "    axes[1, 0].set_xscale('log')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Hidden Size vs Validation Loss\n",
    "if completed_trials:\n",
    "    hidden_sizes = [t.params.get('hidden_size', 0) for t in completed_trials]\n",
    "    axes[1, 1].scatter(hidden_sizes, val_losses, alpha=0.6)\n",
    "    axes[1, 1].set_xlabel('Hidden Size')\n",
    "    axes[1, 1].set_ylabel('Validation Loss')\n",
    "    axes[1, 1].set_title('Hidden Size vs Validation Loss')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Salva study results in JSON per analisi future\n",
    "study_results = {\n",
    "    'best_trial_number': best_trial.number,\n",
    "    'best_validation_loss': best_trial.value,\n",
    "    'best_params': best_trial.params,\n",
    "    'n_trials': len(study.trials),\n",
    "    'n_completed': len(completed_trials),\n",
    "    'n_failed': len(failed_trials),\n",
    "    'n_pruned': len(pruned_trials)\n",
    "}\n",
    "\n",
    "with open('optuna_study_results.json', 'w') as f:\n",
    "    json.dump(study_results, f, indent=4)\n",
    "\n",
    "print(f\"\\n‚úì Studio Optuna salvato in 'optuna_study_results.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01e05a2",
   "metadata": {},
   "source": [
    "## 7. Best Model Selection & Final Evaluation\n",
    "\n",
    "Selezione del miglior modello e valutazione finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68573073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training del miglior modello con configurazione completa\n",
    "def train_best_model(best_params):\n",
    "    \"\"\"\n",
    "    Allena il miglior modello con gli iperparametri ottimali\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"TRAINING FINAL MODEL CON BEST HYPERPARAMETERS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Crea dataset con migliori parametri\n",
    "    training_dataset = TimeSeriesDataSet(\n",
    "        train_data,\n",
    "        time_idx=\"time_idx\",\n",
    "        target=\"power_kw\",\n",
    "        group_ids=[\"group_id\"],\n",
    "        min_encoder_length=MAX_ENCODER_LENGTH,\n",
    "        max_encoder_length=MAX_ENCODER_LENGTH,\n",
    "        min_prediction_length=MAX_PREDICTION_LENGTH,\n",
    "        max_prediction_length=MAX_PREDICTION_LENGTH,\n",
    "        static_categoricals=[\"group_id\"],\n",
    "        time_varying_known_reals=known_reals,\n",
    "        time_varying_unknown_reals=[\"power_kw\"],\n",
    "        target_normalizer=GroupNormalizer(groups=[\"group_id\"], transformation=None),\n",
    "        add_relative_time_idx=True,\n",
    "        add_target_scales=True,\n",
    "        add_encoder_length=True,\n",
    "    )\n",
    "    \n",
    "    validation_dataset = TimeSeriesDataSet.from_dataset(\n",
    "        training_dataset, processed_data, predict=True, stop_randomization=True\n",
    "    )\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_dataloader = training_dataset.to_dataloader(\n",
    "        train=True, batch_size=best_params['batch_size'], num_workers=0\n",
    "    )\n",
    "    val_dataloader = validation_dataset.to_dataloader(\n",
    "        train=False, batch_size=best_params['batch_size'] * 2, num_workers=0\n",
    "    )\n",
    "    \n",
    "    # Modello TFT con migliori parametri\n",
    "    tft = TemporalFusionTransformer.from_dataset(\n",
    "        training_dataset,\n",
    "        learning_rate=best_params['learning_rate'],\n",
    "        hidden_size=best_params['hidden_size'],\n",
    "        lstm_layers=best_params['lstm_layers'],\n",
    "        attention_head_size=best_params['attention_head_size'],\n",
    "        dropout=best_params['dropout'],\n",
    "        hidden_continuous_size=best_params['hidden_continuous_size'],\n",
    "        output_size=7,\n",
    "        loss=QuantileLoss(),\n",
    "        log_interval=10,\n",
    "        reduce_on_plateau_patience=4,\n",
    "    )\n",
    "    \n",
    "    # Callbacks per training finale\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=1e-4,\n",
    "        patience=best_params['patience'],\n",
    "        verbose=True,\n",
    "        mode=\"min\"\n",
    "    )\n",
    "    \n",
    "    lr_monitor = LearningRateMonitor(logging_interval=\"epoch\")\n",
    "    logger = TensorBoardLogger(\"lightning_logs\", name=\"best_tft_model\")\n",
    "    \n",
    "    # Trainer per modello finale (pi√π epochs)\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=150,  # Pi√π epochs per il modello finale\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        gradient_clip_val=best_params['gradient_clip_val'],\n",
    "        callbacks=[early_stop_callback, lr_monitor],\n",
    "        logger=logger,\n",
    "        enable_progress_bar=True,\n",
    "        enable_model_summary=True,\n",
    "    )\n",
    "    \n",
    "    print(\"Inizio training del modello finale...\")\n",
    "    trainer.fit(tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n",
    "    \n",
    "    return trainer, tft, val_dataloader\n",
    "\n",
    "# Training modello finale usando l'ultimo fold (pi√π dati)\n",
    "best_params = study.best_trial.params\n",
    "train_data, val_data = folds[-1]  # Usa l'ultimo fold con pi√π dati di training\n",
    "final_trainer, final_model, final_val_dataloader = train_best_model(best_params)\n",
    "\n",
    "print(f\"\\n‚úì Training completato!\")\n",
    "print(f\"Best model path: {final_trainer.checkpoint_callback.best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0301d11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valutazione finale del miglior modello\n",
    "def evaluate_final_model(trainer, model, val_dataloader):\n",
    "    \"\"\"\n",
    "    Valutazione finale con metriche complete\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"VALUTAZIONE FINALE DEL MIGLIOR MODELLO\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Carica best checkpoint\n",
    "    best_model = TemporalFusionTransformer.load_from_checkpoint(\n",
    "        trainer.checkpoint_callback.best_model_path\n",
    "    )\n",
    "    \n",
    "    # Predizioni\n",
    "    predictions = best_model.predict(\n",
    "        val_dataloader, \n",
    "        mode=\"prediction\",\n",
    "        return_x=True,\n",
    "        trainer_kwargs=dict(accelerator=\"auto\"),\n",
    "    )\n",
    "    \n",
    "    # Estrai dati\n",
    "    pred_output = predictions.output\n",
    "    decoder_target = predictions.x[\"decoder_target\"]\n",
    "    \n",
    "    # Gestisci diverse forme di output\n",
    "    if len(pred_output.shape) == 2 and pred_output.shape[1] == 24:\n",
    "        y_pred = pred_output.cpu().numpy()\n",
    "        y_true = decoder_target.cpu().numpy()\n",
    "    elif len(pred_output.shape) == 3:\n",
    "        median_idx = pred_output.shape[2] // 2\n",
    "        y_pred = pred_output[:, :, median_idx].cpu().numpy()\n",
    "        y_true = decoder_target.cpu().numpy()\n",
    "    else:\n",
    "        raise ValueError(f\"Shape inattesa per predictions: {pred_output.shape}\")\n",
    "    \n",
    "    # Calcola metriche\n",
    "    y_true_flat = y_true.flatten()\n",
    "    y_pred_flat = y_pred.flatten()\n",
    "    \n",
    "    mae = mean_absolute_error(y_true_flat, y_pred_flat)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true_flat, y_pred_flat))\n",
    "    mape = np.mean(np.abs((y_true_flat - y_pred_flat) / (y_true_flat + 1e-8))) * 100\n",
    "    r2 = r2_score(y_true_flat, y_pred_flat)\n",
    "    \n",
    "    # Final metrics\n",
    "    final_metrics = {\n",
    "        'MAE': float(mae),\n",
    "        'RMSE': float(rmse),\n",
    "        'MAPE': float(mape),\n",
    "        'R2': float(r2),\n",
    "        'val_loss': float(trainer.callback_metrics.get(\"val_loss\", 0)),\n",
    "        'best_hyperparameters': best_params,\n",
    "        'training_samples': len(train_data),\n",
    "        'validation_samples': len(val_data),\n",
    "        'encoder_length': MAX_ENCODER_LENGTH,\n",
    "        'prediction_length': MAX_PREDICTION_LENGTH,\n",
    "    }\n",
    "    \n",
    "    print(f\"METRICHE FINALI DEL MIGLIOR MODELLO:\")\n",
    "    print(f\"  MAE:  {mae:.6f} kW\")\n",
    "    print(f\"  RMSE: {rmse:.6f} kW\")\n",
    "    print(f\"  MAPE: {mape:.4f}%\")\n",
    "    print(f\"  R¬≤:   {r2:.6f}\")\n",
    "    print(f\"  Val Loss: {final_metrics['val_loss']:.6f}\")\n",
    "    \n",
    "    return final_metrics, y_pred, y_true\n",
    "\n",
    "# Valutazione\n",
    "final_metrics, y_pred_final, y_true_final = evaluate_final_model(\n",
    "    final_trainer, final_model, final_val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b542bd63",
   "metadata": {},
   "source": [
    "## 8. Save Best Hyperparameters\n",
    "\n",
    "Salvataggio degli iperparametri ottimali in file di testo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db7006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio dei migliori iperparametri in file di testo\n",
    "def save_best_hyperparameters(final_metrics, study_info):\n",
    "    \"\"\"\n",
    "    Salva i migliori iperparametri e metriche in un file di testo dettagliato\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    filename = f\"best_hyperparameters_{timestamp}.txt\"\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(\"TEMPORAL FUSION TRANSFORMER - BEST HYPERPARAMETERS\\n\")\n",
    "        f.write(\"Photovoltaic Power Forecasting (24h ahead)\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Hyperparameter tuning completed with {study_info['n_trials']} trials\\n\\n\")\n",
    "        \n",
    "        # Informazioni dataset\n",
    "        f.write(\"DATASET INFORMATION:\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        f.write(f\"Training samples: {final_metrics['training_samples']:,}\\n\")\n",
    "        f.write(f\"Validation samples: {final_metrics['validation_samples']:,}\\n\")\n",
    "        f.write(f\"Total samples: {final_metrics['training_samples'] + final_metrics['validation_samples']:,}\\n\")\n",
    "        f.write(f\"Train/Val split: 80%/20% (temporal)\\n\")\n",
    "        f.write(f\"Encoder length: {final_metrics['encoder_length']} hours (1 week)\\n\")\n",
    "        f.write(f\"Prediction length: {final_metrics['prediction_length']} hours (1 day)\\n\\n\")\n",
    "        \n",
    "        # Migliori iperparametri\n",
    "        f.write(\"OPTIMAL HYPERPARAMETERS:\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        hyperparams = final_metrics['best_hyperparameters']\n",
    "        \n",
    "        f.write(\"Model Architecture:\\n\")\n",
    "        f.write(f\"  hidden_size: {hyperparams['hidden_size']}\\n\")\n",
    "        f.write(f\"  lstm_layers: {hyperparams['lstm_layers']}\\n\")\n",
    "        f.write(f\"  attention_head_size: {hyperparams['attention_head_size']}\\n\")\n",
    "        f.write(f\"  hidden_continuous_size: {hyperparams['hidden_continuous_size']}\\n\")\n",
    "        f.write(f\"  output_size: 7 (fixed - quantile loss)\\n\\n\")\n",
    "        \n",
    "        f.write(\"Training Configuration:\\n\")\n",
    "        f.write(f\"  learning_rate: {hyperparams['learning_rate']:.6f}\\n\")\n",
    "        f.write(f\"  batch_size: {hyperparams['batch_size']}\\n\")\n",
    "        f.write(f\"  dropout: {hyperparams['dropout']:.3f}\\n\")\n",
    "        f.write(f\"  gradient_clip_val: {hyperparams['gradient_clip_val']:.3f}\\n\")\n",
    "        f.write(f\"  patience (early stopping): {hyperparams['patience']}\\n\")\n",
    "        f.write(f\"  max_epochs: 150 (final model)\\n\")\n",
    "        f.write(f\"  loss_function: QuantileLoss\\n\\n\")\n",
    "        \n",
    "        # Feature configuration\n",
    "        f.write(\"FEATURE CONFIGURATION:\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        f.write(\"Time-varying known reals:\\n\")\n",
    "        for feature in known_reals:\n",
    "            f.write(f\"  - {feature}\\n\")\n",
    "        f.write(\"\\nTime-varying unknown reals:\\n\")\n",
    "        f.write(\"  - power_kw (target variable)\\n\")\n",
    "        f.write(\"\\nStatic categoricals:\\n\")\n",
    "        f.write(\"  - group_id\\n\\n\")\n",
    "        \n",
    "        # Performance metrics\n",
    "        f.write(\"PERFORMANCE METRICS:\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        f.write(f\"Validation Loss: {final_metrics['val_loss']:.6f}\\n\")\n",
    "        f.write(f\"MAE (Mean Absolute Error): {final_metrics['MAE']:.6f} kW\\n\")\n",
    "        f.write(f\"RMSE (Root Mean Squared Error): {final_metrics['RMSE']:.6f} kW\\n\")\n",
    "        f.write(f\"MAPE (Mean Absolute Percentage Error): {final_metrics['MAPE']:.4f}%\\n\")\n",
    "        f.write(f\"R¬≤ (Coefficient of Determination): {final_metrics['R2']:.6f}\\n\\n\")\n",
    "        \n",
    "        # Optuna study info\n",
    "        f.write(\"HYPERPARAMETER TUNING DETAILS:\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        f.write(f\"Total trials executed: {study_info['n_trials']}\\n\")\n",
    "        f.write(f\"Completed trials: {study_info['n_completed']}\\n\")\n",
    "        f.write(f\"Failed trials: {study_info['n_failed']}\\n\")\n",
    "        f.write(f\"Pruned trials: {study_info['n_pruned']}\\n\")\n",
    "        f.write(f\"Best trial number: #{study_info['best_trial_number']}\\n\")\n",
    "        f.write(f\"Best validation loss: {study_info['best_validation_loss']:.6f}\\n\")\n",
    "        f.write(f\"Optimization algorithm: TPE Sampler\\n\")\n",
    "        f.write(f\"Pruning algorithm: Median Pruner\\n\\n\")\n",
    "        \n",
    "        # Usage instructions\n",
    "        f.write(\"USAGE INSTRUCTIONS:\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        f.write(\"To reproduce this model, use the hyperparameters above with:\\n\")\n",
    "        f.write(\"1. Load and preprocess data with 168h encoder / 24h prediction\\n\")\n",
    "        f.write(\"2. Create TimeSeriesDataSet with the specified features\\n\")\n",
    "        f.write(\"3. Initialize TemporalFusionTransformer with these hyperparameters\\n\")\n",
    "        f.write(\"4. Train with EarlyStopping (patience as specified)\\n\")\n",
    "        f.write(\"5. Use QuantileLoss for probabilistic forecasting\\n\\n\")\n",
    "        \n",
    "        # Additional notes\n",
    "        f.write(\"NOTES:\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        f.write(\"- This configuration was optimized using Optuna Bayesian optimization\\n\")\n",
    "        f.write(\"- Temporal cross-validation was used to prevent data leakage\\n\")\n",
    "        f.write(\"- The model provides 24-hour ahead forecasting with uncertainty quantification\\n\")\n",
    "        f.write(\"- Performance metrics are calculated on the held-out validation set\\n\")\n",
    "        f.write(\"- For production use, consider retraining on the full dataset\\n\\n\")\n",
    "        \n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(\"END OF HYPERPARAMETER CONFIGURATION\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    return filename\n",
    "\n",
    "# Prepara info per salvataggio\n",
    "study_info = {\n",
    "    'n_trials': len(study.trials),\n",
    "    'n_completed': len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]),\n",
    "    'n_failed': len([t for t in study.trials if t.state == optuna.trial.TrialState.FAIL]),\n",
    "    'n_pruned': len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]),\n",
    "    'best_trial_number': study.best_trial.number,\n",
    "    'best_validation_loss': study.best_value\n",
    "}\n",
    "\n",
    "# Salva file di testo\n",
    "txt_filename = save_best_hyperparameters(final_metrics, study_info)\n",
    "\n",
    "# Salva anche JSON per uso programmatico\n",
    "json_filename = f\"best_hyperparameters_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.json\"\n",
    "with open(json_filename, 'w') as f:\n",
    "    json.dump(final_metrics, f, indent=4)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SALVATAGGIO COMPLETATO\")\n",
    "print(\"=\"*70)\n",
    "print(f\"‚úÖ File di testo salvato: {txt_filename}\")\n",
    "print(f\"‚úÖ File JSON salvato: {json_filename}\")\n",
    "print(f\"‚úÖ Studio Optuna: optuna_study_results.json\")\n",
    "print(\"\\nI file contengono:\")\n",
    "print(\"  - Configurazione completa del miglior modello\")\n",
    "print(\"  - Iperparametri ottimali\")\n",
    "print(\"  - Metriche di performance\")\n",
    "print(\"  - Istruzioni per riprodurre il modello\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69db3a1c",
   "metadata": {},
   "source": [
    "## üìä Summary - Hyperparameter Tuning Completato\n",
    "\n",
    "### ‚úÖ Pipeline Eseguita con Successo\n",
    "\n",
    "**Obiettivo raggiunto:** Ottimizzazione completa degli iperparametri del TFT per previsione fotovoltaica a 24h\n",
    "\n",
    "### üîß Configurazione Utilizzata\n",
    "\n",
    "- **Dataset:** 100% utilizzato con temporal cross-validation 80/20\n",
    "- **Ottimizzazione:** Optuna TPE Sampler + Median Pruner  \n",
    "- **Spazio di ricerca:** 9 iperparametri chiave (architettura + training)\n",
    "- **Trials:** 50 esperimenti automatizzati con early stopping\n",
    "\n",
    "### üìà Risultati Ottenuti\n",
    "\n",
    "I migliori iperparametri sono stati salvati nei seguenti file:\n",
    "- **File di testo dettagliato** con configurazione completa\n",
    "- **File JSON** per uso programmatico\n",
    "- **Studio Optuna** per analisi avanzate\n",
    "\n",
    "### üéØ Utilizzo dei Risultati\n",
    "\n",
    "I file salvati contengono:\n",
    "1. **Iperparametri ottimali** per tutti i componenti TFT\n",
    "2. **Metriche di performance** validate\n",
    "3. **Configurazione completa** per riprodurre il modello\n",
    "4. **Istruzioni dettagliate** per l'implementazione\n",
    "\n",
    "### üöÄ Prossimi Passi\n",
    "\n",
    "Utilizzare gli iperparametri ottimali nel notebook principale per:\n",
    "- Training del modello di produzione\n",
    "- Valutazione finale su test set\n",
    "- Deploy del sistema di forecasting\n",
    "\n",
    "---\n",
    "**Hyperparameter tuning completato con successo! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
